{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instructions.chatbot\n",
      "instructions.chatbot\n",
      "torch._utils\n",
      "torch.storage\n",
      "collections\n"
     ]
    }
   ],
   "source": [
    "from baseline_chatbot import load_arena_pickle\n",
    "pickle_path = '../../../ARENA_2.0/chapter1_transformers/instructions/my_embeddings.pkl'\n",
    "my_embeddings = load_arena_pickle(pickle_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install trulens_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "tru = Tru()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_chatbot import prompt_templates_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_embeddings\n",
    "model: str = \"gpt-4-1106-preview\"\n",
    "question: str = \"What is an example question which you can answer for me?\"\n",
    "prompt_template: str = \"SIMPLE\"\n",
    "prompt_templates_dict = prompt_templates_dict\n",
    "max_len: int = 1800\n",
    "engine: str = 'text-embedding-ada-002'\n",
    "max_tokens: int = 200\n",
    "stop_sequence = None\n",
    "debug: bool = False\n",
    "temperature: float = 0.\n",
    "container = None\n",
    "is_streamlit=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import  OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine: str = 'text-embedding-ada-002'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Why attention is so useful?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_chatbot import create_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = create_context(\n",
    "        question,\n",
    "        my_embeddings,\n",
    "        max_len=max_len,\n",
    "        engine=engine,\n",
    "        debug=debug,\n",
    "        llm_client=client\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineRAGWrapper:\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        context = create_context(\n",
    "                question,\n",
    "                my_embeddings,\n",
    "                max_len=max_len,\n",
    "                engine=engine,\n",
    "                debug=debug,\n",
    "                llm_client=client\n",
    "        )\n",
    "\n",
    "        return context\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, question: str, context: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        actual_prompt_template = prompt_templates_dict[prompt_template]\n",
    "        prompt = actual_prompt_template.format(question=question, context=context)\n",
    "        if debug:\n",
    "            print(f\"Context length: {len(context)}\")\n",
    "            print(\"\\n\\n\")\n",
    "            print(f\"Responding to prompt:\\n\\n{prompt}\")\n",
    "        kwargs = dict(\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=stop_sequence,\n",
    "            model=model,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        completion = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": prompt}], **kwargs)\n",
    "        completion = completion.choices[0].message.content\n",
    "        return completion\n",
    "\n",
    "    @instrument\n",
    "    def query(self, question: str) -> str:\n",
    "        context_str = self.retrieve(question)\n",
    "        completion = self.generate_completion(question, context_str)\n",
    "        return completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = BaselineRAGWrapper().query(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, I can't answer this based on the context, because the question \"Why attention is so useful?\" is quite broad and general, while the context provided is specifically about the technical details and analysis of attention mechanisms within transformer models. The context discusses different types of attention (causal vs bidirectional), attention patterns, and the interpretability of attention heads, but it does not directly address the general utility of attention mechanisms. However, I can still provide an answer based on my knowledge.\n",
      "\n",
      "Attention mechanisms are useful in neural network models, particularly in transformers, for several reasons:\n",
      "\n",
      "1. **Contextual Understanding**: Attention allows the model to focus on different parts of the input sequence when making predictions, which is crucial for tasks like language understanding where the relevance of words can depend heavily on context.\n",
      "\n",
      "2. **Dynamic Weighting**: Unlike convolutional or recurrent layers, attention does not treat all parts of the input equally but assigns different weights dynamically, which can lead to more nuanced and accurate representations.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(completion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
