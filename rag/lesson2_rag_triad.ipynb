{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import Document\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"./data/dynalist-2023-12-8.txt\"]\n",
    ").load_data()\n",
    "\n",
    "document = Document(text=\"\\n\\n\".join([doc.text for doc in documents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255302"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_sentence_window_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "sentence_index = build_sentence_window_index(\n",
    "    document,\n",
    "    llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\",\n",
    "    save_dir=\"sentence_index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_eval_questions\n",
    "eval_questions = read_eval_questions()\n",
    "eval_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2f0cd2366547cc8d02bd234535ad86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/799 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc89fc11eb048f2a8a673922a750181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaa0c3775914db6848d58e2f89c6c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b0bb9e278e497799ca8cd722c9e9f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1242981e8d49e6a4f946627163aced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import get_sentence_window_query_engine\n",
    "sentence_window_engine = get_sentence_window_query_engine(sentence_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Mechanistic interpretability refers to the field of study that focuses on reverse engineering neural networks from the learned weights to human-interpretable algorithms. It involves understanding the actual mechanisms and algorithms that make up the network. In contrast to other forms of interpretability, which explain how the network's outputs relate to high-level concepts without referencing the network's functioning, mechanistic interpretability aims to uncover the inner workings of the network.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = sentence_window_engine.query(\n",
    "    \"What is mechanistic interpretability?\")\n",
    "output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import OpenAI as fOpenAI\n",
    "\n",
    "provider = fOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Answer Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval import Feedback\n",
    "\n",
    "f_qa_relevance = Feedback(\n",
    "    provider.relevance_with_cot_reasons,\n",
    "    name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Context Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruLlama\n",
    "\n",
    "context_selection = TruLlama.select_source_nodes().node.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "f_qs_relevance = (\n",
    "    Feedback(provider.qs_relevance,\n",
    "             name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context_selection)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input statement will be set to __record__.app.query.rets.source_nodes[:].node.text .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "f_qs_relevance = (\n",
    "    Feedback(provider.qs_relevance_with_cot_reasons,\n",
    "             name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(context_selection)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Groundedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "grounded = Groundedness(groundedness_provider=provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "f_groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons,\n",
    "             name=\"Groundedness\"\n",
    "            )\n",
    "    .on(context_selection)\n",
    "    .on_output()\n",
    "    .aggregate(grounded.grounded_statements_aggregator)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruLlama\n",
    "from trulens_eval import FeedbackMode\n",
    "\n",
    "tru_recorder = TruLlama(\n",
    "    sentence_window_engine,\n",
    "    app_id=\"00_Mech_Interpretability\",\n",
    "    feedbacks=[\n",
    "        f_qa_relevance,\n",
    "        f_qs_relevance,\n",
    "        f_groundedness\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3,botocore is/are required for using BedrockEndpoint. You should be able to install it/them with\n",
      "\tpip install boto3 botocore\n",
      "Mechanistic interpretability refers to the field of study that focuses on reverse engineering neural networks from the learned weights to human-interpretable algorithms. It involves understanding the actual mechanisms and algorithms that make up the network. In contrast to other forms of interpretability, which explain how the network's outputs relate to high-level concepts without referencing the network's functioning, mechanistic interpretability aims to uncover the inner workings of the network.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, mechanistic interpretability is applicable to the real world. Mechanistic interpretability is a field of study that focuses on reverse engineering neural networks to understand the underlying mechanisms and algorithms that compose them. This approach aims to make neural networks more interpretable and understandable to humans. By gaining insights into the inner workings of neural networks, researchers can better understand why AI systems make certain decisions and how they arrive at their outputs. This understanding can have real-world applications in various domains, such as healthcare, finance, and autonomous systems, where interpretability and transparency are crucial for trust, accountability, and safety.\n",
      "\n",
      "Superposition refers to a situation where a model represents more features than the number of dimensions in its activation space. In other words, the model is able to simulate a larger model by using a set of interpretable directions that is larger than the number of dimensions. This set of directions is called an overcomplete basis. It is important to note that in the case of superposition, there cannot be an interpretable basis, meaning that features as neurons cannot perfectly hold. To understand superposition, it is helpful to think of it as a form of lossy compression, where the model is able to represent more features but at the cost of adding noise and interference between features. Finding the optimal balance between representing more features and minimizing noise and interference is crucial.\n",
      "\n",
      "Activation functions can play a role in the interpretability of a model. In the context provided, the SoLU activation function is mentioned as a function that seems to make neurons more interpretable. It is suggested that using SoLU as a replacement for other activation functions like GELU or ReLU can reduce the amount of neuron superposition in the model and make neurons more monosemantic. This can potentially make it easier to identify and understand the specific contributions of individual neurons to the model's computations. By localizing the effects of different activations and identifying which parts of the model matter for specific tasks, it becomes possible to form a clean mechanistic story and reverse engineer the underlying circuit represented by the model. Therefore, activation functions can have an impact on the interpretability of a model by influencing the behavior and characteristics of individual neurons.\n",
      "\n",
      "A feature is a property of an input to a model or some subset of that input. It can be a meaningful and articulable property of the input that the network encodes as a direction in activation space. Features can vary depending on the type of model being used, such as curve detector or car detector neurons in a convolutional neural network. However, the concept of a feature is not limited to human-understandable properties and can encompass any \"independent units\" that a neural network representation can be decomposed into. Defining a feature in a satisfying way can be challenging, but it is an important aspect of understanding and interpreting machine learning models.\n",
      "\n",
      "Activation refers to the intermediate values computed when running a neural network, specifically the outputs of each layer. On the other hand, a feature can be defined as an \"independent unit\" that a neural network representation can be decomposed into. In other words, features are the meaningful, articulable properties of the input that the network encodes as directions in activation space. While activations are the values themselves, features are the properties or characteristics that these values represent.\n",
      "\n",
      "A circuit, in the context provided, refers to a computational subgraph within a model. It is a subset of nodes and edges that are sufficient for performing a specific computation. In this framework, nodes represent components of the model, such as attention heads and neurons, while edges represent the flow of information between these components. The output of each layer in the model is the sum of the outputs of its components, and the input to each layer is the sum of the outputs of every previous layer. This allows for the consideration of subsets of nodes and edges, making it easy to understand the effect of adding or removing terms. Overall, a circuit represents a part of the model that performs a comprehensible computation to generate interpretable features.\n",
      "\n",
      "An Induction Head is a type of head that implements the induction behavior. It attends to the token immediately after an earlier copy of the current token and predicts that the token attended to will come next. It is a statement about the attention pattern and does not provide information about the output of the head (OV circuit).\n",
      "\n",
      "The induction heads were discovered by studying two-layer attention-only models. These models provided a simpler setting to analyze compared to real language models. Through this study, a deep principle of transformers was uncovered, which turned out to be generalizable. The discovery of induction heads in these toy models has sparked excitement and the desire for further research in studying similar models and exploring what can be learned from them.\n",
      "\n",
      "Induction heads are important because they play a crucial role in language modeling and in-context learning. They allow transformers to use tokens from far back in the context to predict the next token, which improves the model's ability to understand and generate coherent and contextually relevant language. This is why there is a visible bump in the loss curve when induction heads form during training.\n",
      "\n",
      "Curve circuits and Indirect Object Identification Circuit are other examples of found circuits, in addition to Induction Head.\n",
      "\n",
      "Sparsity refers to how frequently a feature is present in the input. In the context provided, it is mentioned that controlling for importance, if a feature is sparse, it will interfere with other features less. This suggests that sparsity inhibits superposition, as most features are sparse and not present in most inputs.\n",
      "\n",
      "The phase transition refers to a sudden development of some capability in a model during a brief period of training. It can occur over training, dataset size, and model size/scale. Examples of phase transitions include the development of induction heads and the capability of in-context learning in models. These transitions are characterized by S-shaped loss curves, with plateaus, rapid increases or decreases, and then another plateau.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question in eval_questions:\n",
    "    with tru_recorder as recording:\n",
    "        output = sentence_window_engine.query(question)\n",
    "        print(output.response)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>Answer Relevance_calls</th>\n",
       "      <th>Context Relevance_calls</th>\n",
       "      <th>Groundedness_calls</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00_Mech_Interpretability</td>\n",
       "      <td>{\"app_id\": \"00_Mech_Interpretability\", \"tags\":...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_a554613dc706075d8ffb2475485efa03</td>\n",
       "      <td>\"What is mechanistic interpretability?\"</td>\n",
       "      <td>\"Mechanistic interpretability refers to the fi...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_a554613dc706075d8ff...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-08T14:23:38.032519\", \"...</td>\n",
       "      <td>2023-12-08T14:23:41.470001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[{'args': {'prompt': 'What is mechanistic inte...</td>\n",
       "      <td>[{'args': {'question': 'What is mechanistic in...</td>\n",
       "      <td>[{'args': {'source': 'More generally, if somet...</td>\n",
       "      <td>3</td>\n",
       "      <td>970</td>\n",
       "      <td>0.001498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00_Mech_Interpretability</td>\n",
       "      <td>{\"app_id\": \"00_Mech_Interpretability\", \"tags\":...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_e2cf6815609e57f9acf5ebf92126b2e9</td>\n",
       "      <td>\"Is mechanistic interpretability appliable to ...</td>\n",
       "      <td>\"Yes, mechanistic interpretability is applicab...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_e2cf6815609e57f9acf...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-08T14:23:41.647744\", \"...</td>\n",
       "      <td>2023-12-08T14:23:45.612452</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>[{'args': {'prompt': 'Is mechanistic interpret...</td>\n",
       "      <td>[{'args': {'question': 'Is mechanistic interpr...</td>\n",
       "      <td>[{'args': {'source': 'Where possible, I link t...</td>\n",
       "      <td>3</td>\n",
       "      <td>1009</td>\n",
       "      <td>0.001574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00_Mech_Interpretability</td>\n",
       "      <td>{\"app_id\": \"00_Mech_Interpretability\", \"tags\":...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_195632953a5788d3f4014d80bf371b5b</td>\n",
       "      <td>\"What is superposition and how can I understan...</td>\n",
       "      <td>\"Superposition refers to a situation where a m...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_195632953a5788d3f40...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-08T14:23:45.756853\", \"...</td>\n",
       "      <td>2023-12-08T14:23:50.768928</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>[{'args': {'prompt': 'What is superposition an...</td>\n",
       "      <td>[{'args': {'question': 'What is superposition ...</td>\n",
       "      <td>[{'args': {'source': 'We can both use polysema...</td>\n",
       "      <td>5</td>\n",
       "      <td>715</td>\n",
       "      <td>0.001147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00_Mech_Interpretability</td>\n",
       "      <td>{\"app_id\": \"00_Mech_Interpretability\", \"tags\":...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_3fb3f67700492d460b6ef182224eeab5</td>\n",
       "      <td>\"How activation functions relate to interpreta...</td>\n",
       "      <td>\"Activation functions can play a role in the i...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_3fb3f67700492d460b6...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-08T14:23:50.920176\", \"...</td>\n",
       "      <td>2023-12-08T14:23:56.127787</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>[{'args': {'prompt': 'How activation functions...</td>\n",
       "      <td>[{'args': {'question': 'How activation functio...</td>\n",
       "      <td>[{'args': {'source': 'A final takeaway is that...</td>\n",
       "      <td>5</td>\n",
       "      <td>844</td>\n",
       "      <td>0.001350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00_Mech_Interpretability</td>\n",
       "      <td>{\"app_id\": \"00_Mech_Interpretability\", \"tags\":...</td>\n",
       "      <td>RetrieverQueryEngine(llama_index.query_engine....</td>\n",
       "      <td>record_hash_e9993a4ad2473a1963a0e16ddea2c660</td>\n",
       "      <td>\"What is feature?\"</td>\n",
       "      <td>\"A feature is a property of an input to a mode...</td>\n",
       "      <td>-</td>\n",
       "      <td>{\"record_id\": \"record_hash_e9993a4ad2473a1963a...</td>\n",
       "      <td>{\"n_requests\": 1, \"n_successful_requests\": 1, ...</td>\n",
       "      <td>{\"start_time\": \"2023-12-08T14:23:56.269574\", \"...</td>\n",
       "      <td>2023-12-08T14:24:01.023450</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[{'args': {'prompt': 'What is feature?', 'resp...</td>\n",
       "      <td>[{'args': {'question': 'What is feature?', 'st...</td>\n",
       "      <td>[{'args': {'source': 'This isn't necessarily a...</td>\n",
       "      <td>4</td>\n",
       "      <td>875</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     app_id  \\\n",
       "0  00_Mech_Interpretability   \n",
       "1  00_Mech_Interpretability   \n",
       "2  00_Mech_Interpretability   \n",
       "3  00_Mech_Interpretability   \n",
       "4  00_Mech_Interpretability   \n",
       "\n",
       "                                            app_json  \\\n",
       "0  {\"app_id\": \"00_Mech_Interpretability\", \"tags\":...   \n",
       "1  {\"app_id\": \"00_Mech_Interpretability\", \"tags\":...   \n",
       "2  {\"app_id\": \"00_Mech_Interpretability\", \"tags\":...   \n",
       "3  {\"app_id\": \"00_Mech_Interpretability\", \"tags\":...   \n",
       "4  {\"app_id\": \"00_Mech_Interpretability\", \"tags\":...   \n",
       "\n",
       "                                                type  \\\n",
       "0  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "1  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "2  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "3  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "4  RetrieverQueryEngine(llama_index.query_engine....   \n",
       "\n",
       "                                      record_id  \\\n",
       "0  record_hash_a554613dc706075d8ffb2475485efa03   \n",
       "1  record_hash_e2cf6815609e57f9acf5ebf92126b2e9   \n",
       "2  record_hash_195632953a5788d3f4014d80bf371b5b   \n",
       "3  record_hash_3fb3f67700492d460b6ef182224eeab5   \n",
       "4  record_hash_e9993a4ad2473a1963a0e16ddea2c660   \n",
       "\n",
       "                                               input  \\\n",
       "0            \"What is mechanistic interpretability?\"   \n",
       "1  \"Is mechanistic interpretability appliable to ...   \n",
       "2  \"What is superposition and how can I understan...   \n",
       "3  \"How activation functions relate to interpreta...   \n",
       "4                                 \"What is feature?\"   \n",
       "\n",
       "                                              output tags  \\\n",
       "0  \"Mechanistic interpretability refers to the fi...    -   \n",
       "1  \"Yes, mechanistic interpretability is applicab...    -   \n",
       "2  \"Superposition refers to a situation where a m...    -   \n",
       "3  \"Activation functions can play a role in the i...    -   \n",
       "4  \"A feature is a property of an input to a mode...    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_a554613dc706075d8ff...   \n",
       "1  {\"record_id\": \"record_hash_e2cf6815609e57f9acf...   \n",
       "2  {\"record_id\": \"record_hash_195632953a5788d3f40...   \n",
       "3  {\"record_id\": \"record_hash_3fb3f67700492d460b6...   \n",
       "4  {\"record_id\": \"record_hash_e9993a4ad2473a1963a...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "1  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "2  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "3  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "4  {\"n_requests\": 1, \"n_successful_requests\": 1, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2023-12-08T14:23:38.032519\", \"...   \n",
       "1  {\"start_time\": \"2023-12-08T14:23:41.647744\", \"...   \n",
       "2  {\"start_time\": \"2023-12-08T14:23:45.756853\", \"...   \n",
       "3  {\"start_time\": \"2023-12-08T14:23:50.920176\", \"...   \n",
       "4  {\"start_time\": \"2023-12-08T14:23:56.269574\", \"...   \n",
       "\n",
       "                           ts  Answer Relevance  Context Relevance  \\\n",
       "0  2023-12-08T14:23:41.470001               1.0               0.90   \n",
       "1  2023-12-08T14:23:45.612452               1.0               0.80   \n",
       "2  2023-12-08T14:23:50.768928               0.9               0.40   \n",
       "3  2023-12-08T14:23:56.127787               0.9               0.80   \n",
       "4  2023-12-08T14:24:01.023450               0.9               0.85   \n",
       "\n",
       "   Groundedness                             Answer Relevance_calls  \\\n",
       "0      1.000000  [{'args': {'prompt': 'What is mechanistic inte...   \n",
       "1      0.800000  [{'args': {'prompt': 'Is mechanistic interpret...   \n",
       "2      0.833333  [{'args': {'prompt': 'What is superposition an...   \n",
       "3      0.633333  [{'args': {'prompt': 'How activation functions...   \n",
       "4      1.000000  [{'args': {'prompt': 'What is feature?', 'resp...   \n",
       "\n",
       "                             Context Relevance_calls  \\\n",
       "0  [{'args': {'question': 'What is mechanistic in...   \n",
       "1  [{'args': {'question': 'Is mechanistic interpr...   \n",
       "2  [{'args': {'question': 'What is superposition ...   \n",
       "3  [{'args': {'question': 'How activation functio...   \n",
       "4  [{'args': {'question': 'What is feature?', 'st...   \n",
       "\n",
       "                                  Groundedness_calls  latency  total_tokens  \\\n",
       "0  [{'args': {'source': 'More generally, if somet...        3           970   \n",
       "1  [{'args': {'source': 'Where possible, I link t...        3          1009   \n",
       "2  [{'args': {'source': 'We can both use polysema...        5           715   \n",
       "3  [{'args': {'source': 'A final takeaway is that...        5           844   \n",
       "4  [{'args': {'source': 'This isn't necessarily a...        4           875   \n",
       "\n",
       "   total_cost  \n",
       "0    0.001498  \n",
       "1    0.001574  \n",
       "2    0.001147  \n",
       "3    0.001350  \n",
       "4    0.001378  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"What is mechanistic interpretability?\"</td>\n",
       "      <td>\"Mechanistic interpretability refers to the field of study that focuses on reverse engineering neural networks from the learned weights to human-interpretable algorithms. It involves understanding the actual mechanisms and algorithms that make up the network. In contrast to other forms of interpretability, which explain how the network's outputs relate to high-level concepts without referencing the network's functioning, mechanistic interpretability aims to uncover the inner workings of the network.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Is mechanistic interpretability appliable to real world?\"</td>\n",
       "      <td>\"Yes, mechanistic interpretability is applicable to the real world. Mechanistic interpretability is a field of study that focuses on reverse engineering neural networks to understand the underlying mechanisms and algorithms that compose them. This approach aims to make neural networks more interpretable and understandable to humans. By gaining insights into the inner workings of neural networks, researchers can better understand why AI systems make certain decisions and how they arrive at their outputs. This understanding can have real-world applications in various domains, such as healthcare, finance, and autonomous systems, where interpretability and transparency are crucial for trust, accountability, and safety.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"What is superposition and how can I understand it?\"</td>\n",
       "      <td>\"Superposition refers to a situation where a model represents more features than the number of dimensions in its activation space. In other words, the model is able to simulate a larger model by using a set of interpretable directions that is larger than the number of dimensions. This set of directions is called an overcomplete basis. It is important to note that in the case of superposition, there cannot be an interpretable basis, meaning that features as neurons cannot perfectly hold. To understand superposition, it is helpful to think of it as a form of lossy compression, where the model is able to represent more features but at the cost of adding noise and interference between features. Finding the optimal balance between representing more features and minimizing noise and interference is crucial.\"</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"How activation functions relate to interpretability of a model?\"</td>\n",
       "      <td>\"Activation functions can play a role in the interpretability of a model. In the context provided, the SoLU activation function is mentioned as a function that seems to make neurons more interpretable. It is suggested that using SoLU as a replacement for other activation functions like GELU or ReLU can reduce the amount of neuron superposition in the model and make neurons more monosemantic. This can potentially make it easier to identify and understand the specific contributions of individual neurons to the model's computations. By localizing the effects of different activations and identifying which parts of the model matter for specific tasks, it becomes possible to form a clean mechanistic story and reverse engineer the underlying circuit represented by the model. Therefore, activation functions can have an impact on the interpretability of a model by influencing the behavior and characteristics of individual neurons.\"</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"What is feature?\"</td>\n",
       "      <td>\"A feature is a property of an input to a model or some subset of that input. It can be a meaningful and articulable property of the input that the network encodes as a direction in activation space. Features can vary depending on the type of model being used, such as curve detector or car detector neurons in a convolutional neural network. However, the concept of a feature is not limited to human-understandable properties and can encompass any \\\"independent units\\\" that a neural network representation can be decomposed into. Defining a feature in a satisfying way can be challenging, but it is an important aspect of understanding and interpreting machine learning models.\"</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"What is the difference between activation and a feature?\"</td>\n",
       "      <td>\"Activation refers to the intermediate values computed when running a neural network, specifically the outputs of each layer. On the other hand, a feature can be defined as an \\\"independent unit\\\" that a neural network representation can be decomposed into. In other words, features are the meaningful, articulable properties of the input that the network encodes as directions in activation space. While activations are the values themselves, features are the properties or characteristics that these values represent.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"What is a circuit?\"</td>\n",
       "      <td>\"A circuit, in the context provided, refers to a computational subgraph within a model. It is a subset of nodes and edges that are sufficient for performing a specific computation. In this framework, nodes represent components of the model, such as attention heads and neurons, while edges represent the flow of information between these components. The output of each layer in the model is the sum of the outputs of its components, and the input to each layer is the sum of the outputs of every previous layer. This allows for the consideration of subsets of nodes and edges, making it easy to understand the effect of adding or removing terms. Overall, a circuit represents a part of the model that performs a comprehensible computation to generate interpretable features.\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"What is an Induction Head?\"</td>\n",
       "      <td>\"An Induction Head is a type of head that implements the induction behavior. It attends to the token immediately after an earlier copy of the current token and predicts that the token attended to will come next. It is a statement about the attention pattern and does not provide information about the output of the head (OV circuit).\"</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"How Induction Head was found?\"</td>\n",
       "      <td>\"The induction heads were discovered by studying two-layer attention-only models. These models provided a simpler setting to analyze compared to real language models. Through this study, a deep principle of transformers was uncovered, which turned out to be generalizable. The discovery of induction heads in these toy models has sparked excitement and the desire for further research in studying similar models and exploring what can be learned from them.\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Why induction head is so important?\"</td>\n",
       "      <td>\"Induction heads are important because they play a crucial role in language modeling and in-context learning. They allow transformers to use tokens from far back in the context to predict the next token, which improves the model's ability to understand and generate coherent and contextually relevant language. This is why there is a visible bump in the loss curve when induction heads form during training.\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"What are other examples of found circuits except for Induction Head\"</td>\n",
       "      <td>\"Curve circuits and Indirect Object Identification Circuit are other examples of found circuits, in addition to Induction Head.\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"What is sparsity?\"</td>\n",
       "      <td>\"Sparsity refers to how frequently a feature is present in the input. In the context provided, it is mentioned that controlling for importance, if a feature is sparse, it will interfere with other features less. This suggests that sparsity inhibits superposition, as most features are sparse and not present in most inputs.\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"What is the phase transition?\"</td>\n",
       "      <td>\"The phase transition refers to a sudden development of some capability in a model during a brief period of training. It can occur over training, dataset size, and model size/scale. Examples of phase transitions include the development of induction heads and the capability of in-context learning in models. These transitions are characterized by S-shaped loss curves, with plateaus, rapid increases or decreases, and then another plateau.\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    input  \\\n",
       "0                                 \"What is mechanistic interpretability?\"   \n",
       "1              \"Is mechanistic interpretability appliable to real world?\"   \n",
       "2                    \"What is superposition and how can I understand it?\"   \n",
       "3       \"How activation functions relate to interpretability of a model?\"   \n",
       "4                                                      \"What is feature?\"   \n",
       "5              \"What is the difference between activation and a feature?\"   \n",
       "6                                                    \"What is a circuit?\"   \n",
       "7                                            \"What is an Induction Head?\"   \n",
       "8                                         \"How Induction Head was found?\"   \n",
       "9                                   \"Why induction head is so important?\"   \n",
       "10  \"What are other examples of found circuits except for Induction Head\"   \n",
       "11                                                    \"What is sparsity?\"   \n",
       "12                                        \"What is the phase transition?\"   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      output  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                  \"Mechanistic interpretability refers to the field of study that focuses on reverse engineering neural networks from the learned weights to human-interpretable algorithms. It involves understanding the actual mechanisms and algorithms that make up the network. In contrast to other forms of interpretability, which explain how the network's outputs relate to high-level concepts without referencing the network's functioning, mechanistic interpretability aims to uncover the inner workings of the network.\"   \n",
       "1                                                                                                                                                                                                                      \"Yes, mechanistic interpretability is applicable to the real world. Mechanistic interpretability is a field of study that focuses on reverse engineering neural networks to understand the underlying mechanisms and algorithms that compose them. This approach aims to make neural networks more interpretable and understandable to humans. By gaining insights into the inner workings of neural networks, researchers can better understand why AI systems make certain decisions and how they arrive at their outputs. This understanding can have real-world applications in various domains, such as healthcare, finance, and autonomous systems, where interpretability and transparency are crucial for trust, accountability, and safety.\"   \n",
       "2                                                                                                                              \"Superposition refers to a situation where a model represents more features than the number of dimensions in its activation space. In other words, the model is able to simulate a larger model by using a set of interpretable directions that is larger than the number of dimensions. This set of directions is called an overcomplete basis. It is important to note that in the case of superposition, there cannot be an interpretable basis, meaning that features as neurons cannot perfectly hold. To understand superposition, it is helpful to think of it as a form of lossy compression, where the model is able to represent more features but at the cost of adding noise and interference between features. Finding the optimal balance between representing more features and minimizing noise and interference is crucial.\"   \n",
       "3   \"Activation functions can play a role in the interpretability of a model. In the context provided, the SoLU activation function is mentioned as a function that seems to make neurons more interpretable. It is suggested that using SoLU as a replacement for other activation functions like GELU or ReLU can reduce the amount of neuron superposition in the model and make neurons more monosemantic. This can potentially make it easier to identify and understand the specific contributions of individual neurons to the model's computations. By localizing the effects of different activations and identifying which parts of the model matter for specific tasks, it becomes possible to form a clean mechanistic story and reverse engineer the underlying circuit represented by the model. Therefore, activation functions can have an impact on the interpretability of a model by influencing the behavior and characteristics of individual neurons.\"   \n",
       "4                                                                                                                                                                                                                                                                   \"A feature is a property of an input to a model or some subset of that input. It can be a meaningful and articulable property of the input that the network encodes as a direction in activation space. Features can vary depending on the type of model being used, such as curve detector or car detector neurons in a convolutional neural network. However, the concept of a feature is not limited to human-understandable properties and can encompass any \\\"independent units\\\" that a neural network representation can be decomposed into. Defining a feature in a satisfying way can be challenging, but it is an important aspect of understanding and interpreting machine learning models.\"   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                   \"Activation refers to the intermediate values computed when running a neural network, specifically the outputs of each layer. On the other hand, a feature can be defined as an \\\"independent unit\\\" that a neural network representation can be decomposed into. In other words, features are the meaningful, articulable properties of the input that the network encodes as directions in activation space. While activations are the values themselves, features are the properties or characteristics that these values represent.\"   \n",
       "6                                                                                                                                                                    \"A circuit, in the context provided, refers to a computational subgraph within a model. It is a subset of nodes and edges that are sufficient for performing a specific computation. In this framework, nodes represent components of the model, such as attention heads and neurons, while edges represent the flow of information between these components. The output of each layer in the model is the sum of the outputs of its components, and the input to each layer is the sum of the outputs of every previous layer. This allows for the consideration of subsets of nodes and edges, making it easy to understand the effect of adding or removing terms. Overall, a circuit represents a part of the model that performs a comprehensible computation to generate interpretable features.\"   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \"An Induction Head is a type of head that implements the induction behavior. It attends to the token immediately after an earlier copy of the current token and predicts that the token attended to will come next. It is a statement about the attention pattern and does not provide information about the output of the head (OV circuit).\"   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \"The induction heads were discovered by studying two-layer attention-only models. These models provided a simpler setting to analyze compared to real language models. Through this study, a deep principle of transformers was uncovered, which turned out to be generalizable. The discovery of induction heads in these toy models has sparked excitement and the desire for further research in studying similar models and exploring what can be learned from them.\"   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \"Induction heads are important because they play a crucial role in language modeling and in-context learning. They allow transformers to use tokens from far back in the context to predict the next token, which improves the model's ability to understand and generate coherent and contextually relevant language. This is why there is a visible bump in the loss curve when induction heads form during training.\"   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \"Curve circuits and Indirect Object Identification Circuit are other examples of found circuits, in addition to Induction Head.\"   \n",
       "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \"Sparsity refers to how frequently a feature is present in the input. In the context provided, it is mentioned that controlling for importance, if a feature is sparse, it will interfere with other features less. This suggests that sparsity inhibits superposition, as most features are sparse and not present in most inputs.\"   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \"The phase transition refers to a sudden development of some capability in a model during a brief period of training. It can occur over training, dataset size, and model size/scale. Examples of phase transitions include the development of induction heads and the capability of in-context learning in models. These transitions are characterized by S-shaped loss curves, with plateaus, rapid increases or decreases, and then another plateau.\"   \n",
       "\n",
       "    Answer Relevance  Context Relevance  Groundedness  \n",
       "0                1.0               0.90      1.000000  \n",
       "1                1.0               0.80      0.800000  \n",
       "2                0.9               0.40      0.833333  \n",
       "3                0.9               0.80      0.633333  \n",
       "4                0.9               0.85      1.000000  \n",
       "5                1.0               0.55      0.750000  \n",
       "6                1.0                NaN           NaN  \n",
       "7                1.0                NaN           NaN  \n",
       "8                0.0                NaN           NaN  \n",
       "9                NaN                NaN           NaN  \n",
       "10               NaN                NaN           NaN  \n",
       "11               NaN                NaN           NaN  \n",
       "12               NaN                NaN           NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "records[[\"input\", \"output\"] + feedback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Context Relevance</th>\n",
       "      <th>Groundedness</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00_Mech_Interpretability</th>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.836111</td>\n",
       "      <td>3.153846</td>\n",
       "      <td>0.00133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Answer Relevance  Context Relevance  Groundedness  \\\n",
       "app_id                                                                        \n",
       "00_Mech_Interpretability          0.855556           0.716667      0.836111   \n",
       "\n",
       "                           latency  total_cost  \n",
       "app_id                                          \n",
       "00_Mech_Interpretability  3.153846     0.00133  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.get_leaderboard(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c1acb0ce874810859aec60e3f5f127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.68.103:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
